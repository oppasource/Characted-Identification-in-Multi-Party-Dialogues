{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import re\n",
    "import string\n",
    "import os.path\n",
    "import timeit\n",
    "from itertools import chain\n",
    "\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total_utterences: 24396, count_top6: 19817, count_others: 4579\n",
      "\n",
      "**A utterence may contain multiple sentences.\n",
      "\n",
      "total_utterences instances: 24396 \n",
      "count_top6: 19817 \n",
      "count_others: 4579\n",
      "\n",
      "Ross Geller: 3820\n",
      "Rachel Green: 3553\n",
      "Chandler Bing: 3377\n",
      "Monica Geller: 3328\n",
      "Joey Tribbiani: 2907\n",
      "Phoebe Buffay: 2832\n",
      "Others: 4579\n",
      "None: 0\n"
     ]
    }
   ],
   "source": [
    "#Getting top 6 charaters based on utterances\n",
    "\n",
    "characters = {}\n",
    "total_utterences = 0\n",
    "\n",
    "for i in range(1,11):\n",
    "    # Each seasons file path\n",
    "    f = open('json_data/friends_season_'+ str(i).zfill(2) +'.json', 'r')\n",
    "    # Loading seaosns JSON\n",
    "    season = json.loads(f.read())\n",
    "    # Retrieve episodes\n",
    "    episodes = season['episodes']\n",
    "    # Iterate through the episodes\n",
    "    for episode in episodes:\n",
    "        # Retrieve scenes\n",
    "        scenes = episode['scenes']\n",
    "        # Iterate through the scenes\n",
    "        for scene in scenes:\n",
    "            # Retrieve utterances\n",
    "            utterances = scene['utterances']\n",
    "            # Iterate through the utterances\n",
    "            for utterance in utterances:\n",
    "                speaker = utterance['speakers']\n",
    "                \n",
    "                #not considering when multiple speakers are there as it's in less than 1% of cases.\n",
    "                if len(speaker) == 1:\n",
    "                    try:\n",
    "                        #print('\\n\\nSpeaker:',speaker[0], \"Utter_id:\", utterance['utterance_id'], \":-\")\n",
    "                        \n",
    "                        tokens = utterance['tokens']\n",
    "                        speaker = utterance['speakers']\n",
    "                        character_entities = utterance['character_entities']\n",
    "\n",
    "                        total_utterences += 1\n",
    "                        speaker = speaker[0]\n",
    "                        characters[speaker] = characters.get(speaker, 0) + 1\n",
    "                    \n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "# We will use 6 characters who have spoken the most utterances and remaining will be considered in 'others' class\n",
    "entities = [(i[0], i[1]) for i in sorted(characters.items(), key = lambda kv: kv[1], reverse=True)]\n",
    "\n",
    "count_top6 = sum([elem[1] for elem in entities[:6]])\n",
    "count_others = sum([elem[1] for elem in entities[6:]])\n",
    "\n",
    "print(\"\\ntotal_utterences: {0}, count_top6: {1}, count_others: {2}\\n\".format(total_utterences, count_top6, count_others))\n",
    "assert total_utterences == count_top6 + count_others, \"Count Mismatch!!!\"\n",
    "\n",
    "print(\"**A utterence may contain multiple sentences.\")\n",
    "print(\"\\ntotal_utterences instances: {0} \\ncount_top6: {1} \\ncount_others: {2}\\n\".format(total_utterences, \\\n",
    "                                                                                  count_top6, count_others))\n",
    "assert total_utterences == count_top6 + count_others, \"Count Mismatch!!!\"\n",
    "\n",
    "\n",
    "#Taking only top6 and adding two more classes 'Others' and 'None'\n",
    "entities = entities[:6] + [('Others', count_others), ('None', 0)]\n",
    "entities_dict = {i[0]:i[1] for i in entities}\n",
    "\n",
    "entities = [i[0] for i in entities]\n",
    "\n",
    "for e in entities_dict:\n",
    "    print(f'{e}: {entities_dict[e]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to generate tokens: 4.3352 seconds.\n",
      "\n",
      "\n",
      "Total sentences spoken: 26402\n",
      "\n",
      "Following shows count of utterences (an utterence may contain multiple sentences) by top 6 entities and others.      \n",
      "However None shows count of tokens that doesn't refer to any entity (top6 or others).      \n",
      "Need to change this to avoid confusion.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Ross Geller': 3820,\n",
       " 'Rachel Green': 3553,\n",
       " 'Chandler Bing': 3377,\n",
       " 'Monica Geller': 3328,\n",
       " 'Joey Tribbiani': 2907,\n",
       " 'Phoebe Buffay': 2832,\n",
       " 'Others': 4579,\n",
       " 'None': 288967}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Token generation\n",
    "\n",
    "cnt_None = 0\n",
    "data = []\n",
    "labels = []        \n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Sepearte out data and labels\n",
    "for i in range(1,11):\n",
    "    # Each seasons file path\n",
    "    f = open('json_data/friends_season_'+ str(i).zfill(2) +'.json', 'r')\n",
    "    # Loading seaosns JSON\n",
    "    season = json.loads(f.read())\n",
    "    # Retrieve episodes\n",
    "    episodes = season['episodes']\n",
    "    # Iterate through the episodes\n",
    "    for episode in episodes:\n",
    "        # Retrieve scenes\n",
    "        scenes = episode['scenes']\n",
    "        # Iterate through the scenes\n",
    "        for scene in scenes:\n",
    "            # Retrieve utterances\n",
    "            utterances = scene['utterances']\n",
    "            # Iterate through the utterances\n",
    "            for utterance in utterances:\n",
    "                speaker = utterance['speakers']\n",
    "                \n",
    "                if len(speaker) == 1:\n",
    "                    try:\n",
    "                        #print('\\n\\nSpeaker:',speaker[0], \"Utter_id:\", utterance['utterance_id'], \":-\")\n",
    "                        \n",
    "                        tokens = utterance['tokens']\n",
    "                        speaker = utterance['speakers']\n",
    "                        character_entities = utterance['character_entities']\n",
    "\n",
    "                        #coz tokens is list of list where each list represents a sentence.\n",
    "                        for i in range(len(tokens)):\n",
    "                          \n",
    "                            #print(\"\\n\",tokens[i])\n",
    "                            \n",
    "                            #we associate \"None\" label to each token in character_entities\n",
    "                            target = ['None'] * len(tokens[i])\n",
    "                            cnt_None += len(target)\n",
    "                            \n",
    "                            \n",
    "                            \"\"\"\n",
    "                                \"character_entities\": [\n",
    "                                            [],\n",
    "                                            [[0, 1, \"Paul the Wine Guy\"], [4, 5, \"Paul the Wine Guy\"], [5, 6, \"Monica Geller\"]]\n",
    "                                ]\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            #change target labels if character_entities[i] has some entries\n",
    "                            if character_entities[i]:\n",
    "                                for e in character_entities[i]:\n",
    "                                    \n",
    "                                    #start and end index for label\n",
    "                                    indexes = list(range(e[0], e[1]))\n",
    "                                    for j in indexes:\n",
    "                                        if e[2] in entities_dict:\n",
    "                                            target[j] = e[2]\n",
    "                                        else:\n",
    "                                            target[j] = 'Others'\n",
    "                                        \n",
    "                                        #subtracting None count as they are replaced\n",
    "                                        cnt_None -= 1\n",
    "                                        \n",
    "                                    \n",
    "                                # Insert data\n",
    "                                if speaker[0] in entities:\n",
    "                                    data.append({'speaker': speaker[0], 'tokens': tokens[i]})\n",
    "                                    labels.append(target)\n",
    "                                else:\n",
    "                                    data.append({'speaker': 'Others', 'tokens': tokens[i]})\n",
    "                                    labels.append(target)\n",
    "\n",
    "                                #print(\"\\nT:\",data[-1])\n",
    "                                #print(\"L:\",target)\n",
    "                            \n",
    "                            \n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "\n",
    "print(\"\\nTime to generate tokens: %.4f seconds.\\n\" % (timeit.default_timer() - start))\n",
    "\n",
    "print(\"\\nTotal sentences spoken:\", len(data))\n",
    "assert len(data) == len(labels), \"***Size Mismatch!!!***\"\n",
    "\n",
    "\n",
    "print(\"\\nFollowing shows count of utterences (an utterence may contain multiple sentences) by top 6 entities and others.\\\n",
    "      \\nHowever None shows count of tokens that doesn't refer to any entity (top6 or others).\\\n",
    "      \\nNeed to change this to avoid confusion.\")\n",
    "\n",
    "#adding count of None classes\n",
    "entities_dict['None'] = cnt_None\n",
    "entities_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'speaker': 'Monica Geller', 'tokens': ['He', \"'s\", 'just', 'some', 'guy', 'I', 'work', 'with', '!']}\n",
      "['Others', 'None', 'None', 'None', 'Others', 'Monica Geller', 'None', 'None', 'None']\n",
      "\n",
      "{'speaker': 'Joey Tribbiani', 'tokens': ['There', \"'s\", 'got', 'ta', 'be', 'something', 'wrong', 'with', 'him', '!']}\n",
      "['None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'Others', 'None']\n",
      "\n",
      "{'speaker': 'Chandler Bing', 'tokens': ['I', \"'m\", 'still', 'on', 'London', 'time', ',', 'does', 'that', 'count', '?']}\n",
      "['Chandler Bing', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None', 'None']\n"
     ]
    }
   ],
   "source": [
    "print(data[0]) \n",
    "print(labels[0]) \n",
    "\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(data[2]) \n",
    "print(labels[2])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(data[-1]) \n",
    "print(labels[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None              228134\n",
       "Others             20937\n",
       "Ross Geller         7652\n",
       "Rachel Green        5454\n",
       "Chandler Bing       4815\n",
       "Joey Tribbiani      4685\n",
       "Monica Geller       4099\n",
       "Phoebe Buffay       4069\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of each label\n",
    "pd.Series(list(chain.from_iterable(labels))).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean length: 10.599386410120445\n",
      "meadian length: 9.0 \n",
      "\n",
      "6     2397\n",
      "7     2226\n",
      "5     2220\n",
      "8     2088\n",
      "4     1997\n",
      "9     1839\n",
      "10    1731\n",
      "11    1384\n",
      "12    1176\n",
      "3     1060\n",
      "13    1046\n",
      "14     889\n",
      "15     786\n",
      "2      716\n",
      "16     673\n",
      "17     568\n",
      "18     485\n",
      "19     405\n",
      "20     365\n",
      "21     311\n",
      "22     281\n",
      "23     203\n",
      "25     199\n",
      "24     193\n",
      "26     189\n",
      "28     116\n",
      "27     114\n",
      "29     107\n",
      "30      89\n",
      "31      77\n",
      "      ... \n",
      "38      27\n",
      "40      22\n",
      "39      19\n",
      "41      19\n",
      "43      16\n",
      "42      12\n",
      "44      12\n",
      "45      12\n",
      "46       6\n",
      "51       5\n",
      "49       4\n",
      "55       4\n",
      "50       4\n",
      "56       3\n",
      "1        3\n",
      "47       3\n",
      "57       2\n",
      "58       2\n",
      "54       2\n",
      "48       1\n",
      "64       1\n",
      "61       1\n",
      "59       1\n",
      "65       1\n",
      "76       1\n",
      "91       1\n",
      "53       1\n",
      "52       1\n",
      "84       1\n",
      "95       1\n",
      "Length: 66, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# length of each sentence\n",
    "\n",
    "plt.hist([len(r['tokens']) for r in data], bins=50)\n",
    "plt.show()\n",
    "\n",
    "print(\"mean length:\",pd.Series([len(r['tokens']) for r in data]).mean())\n",
    "print(\"meadian length:\", pd.Series([len(r['tokens']) for r in data]).median(), \"\\n\")\n",
    "print(pd.Series([len(r['tokens']) for r in data]).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data \n",
    "dictionary = {\"data\": data,\n",
    "              \"labels\": labels\n",
    "             }\n",
    "\n",
    "output = open(\"dataV3/data_dump.txt\", \"wb\")\n",
    "pickle.dump(dictionary, output)\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda5.2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding present...\n",
      "Time to load: 58.7485 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "#25 dimensional word embeddings\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#If glove embeds is not in word2vec form then first convert it then load it\n",
    "if os.path.isfile('pretrained_embeds/gensim_glove_vectors.txt'):\n",
    "    glove_model = KeyedVectors.load_word2vec_format(\"pretrained_embeds/gensim_glove_vectors.txt\", binary=False)\n",
    "    print(\"Embedding present...\")\n",
    "    print(\"Time to load: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "\n",
    "    \n",
    "else:\n",
    "    glove2word2vec(glove_input_file=\"pretrained_embeds/glove.twitter.27B.25d.txt\", word2vec_output_file=\"pretrained_embeds/gensim_glove_vectors.txt\")\n",
    "    glove_model = KeyedVectors.load_word2vec_format(\"pretrained_embeds/gensim_glove_vectors.txt\", binary=False)\n",
    "    print(\"Time to build and load Word2Vec embeddings from Glove: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "\n",
    "    \n",
    "def get_embed(word):\n",
    "    # Case folding\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        return (glove_model.get_vector(word))\n",
    "    except:\n",
    "        return (glove_model.get_vector('unk'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41681\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#shuffling the data \n",
    "temp = list(zip(data, labels))\n",
    "random.shuffle(temp)\n",
    "data, labels = zip(*temp)\n",
    "\n",
    "data_embedding = []\n",
    "label_index = []\n",
    "\n",
    "#getting utterance embeddings \n",
    "for d in data:\n",
    "    temp = []\n",
    "    \n",
    "    # Getting embeddings for tokens \n",
    "    for t in d['tokens']:\n",
    "        temp.append(get_embed(t))\n",
    "    temp = np.asarray(temp)\n",
    "    \n",
    "    #gettting embeddings for speakers (We are taking average of first name and last name embeddings)\n",
    "    temp_speaker = []\n",
    "    for s in d['speaker'].split():\n",
    "        temp_speaker.append(get_embed(s))\n",
    "    \n",
    "    temp_speaker = np.asarray(temp_speaker)\n",
    "    temp_speaker = np.mean(temp_speaker, axis=0).reshape(1,-1)\n",
    "    \n",
    "    #concatanating speaker embedding with utterance embeddings before the token embeddings\n",
    "    temp = np.vstack((temp_speaker, temp))\n",
    "    data_embedding.append(temp)\n",
    "    \n",
    "    \n",
    "#getting label index from entities list\n",
    "for label in labels:\n",
    "    \n",
    "    #appending a None to compensate for speaker added in embeddings\n",
    "    label = ['None'] + label\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    for l in label:\n",
    "        temp.append(entities.index(l))\n",
    "   \n",
    "    temp = np.asarray(temp)\n",
    "    label_index.append(temp)\n",
    "    \n",
    "\n",
    "print(len(data_embedding))\n",
    "assert len(data_embedding) == len(label_index), \"***Size Mismatch!!!***\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data len: 33344\n",
      "Testing data len: 8337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_SIZE = int(len(data_embedding)*0.8)\n",
    "\n",
    "train_input = data_embedding[:TRAIN_SIZE]\n",
    "train_label_index = label_index[:TRAIN_SIZE]\n",
    "\n",
    "test_input = data_embedding[TRAIN_SIZE:]\n",
    "test_label_index = label_index[TRAIN_SIZE:]\n",
    "\n",
    "print(\"Training data len:\", len(train_input))\n",
    "assert len(train_input) == len(train_label_index), \"***Size Mismatch!!!***\"\n",
    "\n",
    "print(\"Testing data len:\", len(test_input))\n",
    "assert len(test_input) == len(test_label_index), \"***Size Mismatch!!!***\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Rachel Green\n",
      "1 Ross Geller\n",
      "2 Chandler Bing\n",
      "3 Monica Geller\n",
      "4 Joey Tribbiani\n",
      "5 Phoebe Buffay\n",
      "6 Others\n",
      "7 None\n",
      "\n",
      "\n",
      "['Dysprosium', '?']\n",
      "[7 7] \n",
      "\n",
      "['But', 'I', 'wo', \"n't\", '.']\n",
      "[7 5 7 7 7] \n",
      "\n",
      "['And', 'the', 'vet', 'said', 'it', 'was', 'time', '.']\n",
      "[7 7 7 7 6 7 7 7] \n",
      "\n",
      "['Oh', 'yeah', ',', 'but', 'do', \"n't\", 'worry', '.']\n",
      "[7 7 7 7 7 7 7 7] \n",
      "\n",
      "['I', 'know', '!']\n",
      "[0 7 7] \n",
      "\n",
      "['Okay', '.']\n",
      "[7 7] \n",
      "\n",
      "['Who', \"'s\", 'this', 'from', '?']\n",
      "[7 7 7 7 7] \n",
      "\n",
      "['Oh', 'yeah', ',', 'of', 'course', 'you', 'do', \"n't\", '!']\n",
      "[7 7 7 7 7 7 7 7 7] \n",
      "\n",
      "['So', 'you', 'did', \"n't\", 'leave', 'the', 'bank', '?']\n",
      "[7 1 7 7 7 7 7 7] \n",
      "\n",
      "['I', \"'m\", 'sorry', '!']\n",
      "[6 7 7 7] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,e in enumerate(entities):\n",
    "    print(i,e)\n",
    "\n",
    "print('\\n')\n",
    "for i in range(10):\n",
    "    print(data[i][\"tokens\"])\n",
    "    \n",
    "    #coz None is added in front to compensate for \n",
    "    print(train_label_index[i][1:], \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data\n",
    "\n",
    "data_path = 'dataV2/'\n",
    "\n",
    "#np.save() saves in  platform independent format\n",
    "#hot vector of entities labels\n",
    "np.save(data_path + 'entities-vector.npy', entities)\n",
    "\n",
    "#Train Input embeddings\n",
    "np.save(data_path + 'train_input.npy', train_input)\n",
    "\n",
    "#Test input embeddings \n",
    "np.save(data_path + 'test_input.npy', test_input)\n",
    "\n",
    "#Train label indexes\n",
    "np.save(data_path + 'train_label_index.npy', train_label_index)\n",
    "\n",
    "#Train label indexes\n",
    "np.save(data_path + 'test_label_index.npy', test_label_index)\n",
    "\n",
    "\n",
    "#text version of above binary files\n",
    "np.savetxt(data_path + 'entities-vector.txt', entities, fmt = \"%s\")\n",
    "\n",
    "np.savetxt(data_path + 'train_input.txt', train_input, fmt = \"%s\")\n",
    "np.savetxt(data_path + 'test_input.txt', test_input, \"%s\")\n",
    "\n",
    "np.savetxt(data_path + 'train_label_index.txt', train_label_index, fmt = \"%s\" )\n",
    "np.savetxt(data_path + 'test_label_index.txt', test_label_index, fmt = \"%s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
