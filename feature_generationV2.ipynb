{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import os.path\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total_utterences: 61063, count_top6: 51061, count_others: 10002\n",
      "\n",
      "Rachel Green: 9315\n",
      "Ross Geller: 9158\n",
      "Chandler Bing: 8465\n",
      "Monica Geller: 8442\n",
      "Joey Tribbiani: 8177\n",
      "Phoebe Buffay: 7504\n",
      "Others: 10002\n",
      "None: 0\n"
     ]
    }
   ],
   "source": [
    "#Getting top 6 charaters based on utterances\n",
    "\n",
    "characters = {}\n",
    "total_utterences = 0\n",
    "\n",
    "for i in range(1,11):\n",
    "    # Each seasons file path\n",
    "    f = open('json_data/friends_season_'+ str(i).zfill(2) +'.json', 'r')\n",
    "    # Loading seaosns JSON\n",
    "    season = json.loads(f.read())\n",
    "    # Retrieve episodes\n",
    "    episodes = season['episodes']\n",
    "    # Iterate through the episodes\n",
    "    for episode in episodes:\n",
    "        # Retrieve scenes\n",
    "        scenes = episode['scenes']\n",
    "        # Iterate through the scenes\n",
    "        for scene in scenes:\n",
    "            # Retrieve utterances\n",
    "            utterances = scene['utterances']\n",
    "            # Iterate through the utterances\n",
    "            for utterance in utterances:\n",
    "                speaker = utterance['speakers']\n",
    "                \n",
    "                #not considering when multiple speakers are there as it's in less than 1% of cases.\n",
    "                if len(speaker) == 1:\n",
    "                    total_utterences += 1\n",
    "                    speaker = speaker[0]\n",
    "                    characters[speaker] = characters.get(speaker, 0) + 1\n",
    "                    \n",
    "                    \n",
    "\n",
    "# We will use 6 characters who have spoken the most utterances and remaining will be considered in 'others' class\n",
    "entities = [(i[0], i[1]) for i in sorted(characters.items(), key = lambda kv: kv[1], reverse=True)]\n",
    "\n",
    "count_top6 = sum([elem[1] for elem in entities[:6]])\n",
    "count_others = sum([elem[1] for elem in entities[6:]])\n",
    "\n",
    "print(\"\\ntotal_utterences: {0}, count_top6: {1}, count_others: {2}\\n\".format(total_utterences, count_top6, count_others))\n",
    "assert total_utterences == count_top6 + count_others, \"Count Mismatch!!!\"\n",
    "\n",
    "\n",
    "#Taking only top6 and adding two more classes 'Others' and 'None'\n",
    "entities = entities[:6] + [('Others', count_others), ('None', 0)]\n",
    "entities_dict = {i[0]:i[1] for i in entities}\n",
    "\n",
    "entities = [i[0] for i in entities]\n",
    "\n",
    "for e in entities_dict:\n",
    "    print(f'{e}: {entities_dict[e]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to generate tokens: 1.9393 seconds.\n",
      "\n",
      "data len: 41681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Rachel Green': 9315,\n",
       " 'Ross Geller': 9158,\n",
       " 'Chandler Bing': 8465,\n",
       " 'Monica Geller': 8442,\n",
       " 'Joey Tribbiani': 8177,\n",
       " 'Phoebe Buffay': 7504,\n",
       " 'Others': 10002,\n",
       " 'None': 288967}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Token generation\n",
    "\n",
    "cnt_None = 0\n",
    "data = []\n",
    "labels = []        \n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Sepearte out data and labels\n",
    "for i in range(1,11):\n",
    "    # Each seasons file path\n",
    "    f = open('json_data/friends_season_'+ str(i).zfill(2) +'.json', 'r')\n",
    "    # Loading seaosns JSON\n",
    "    season = json.loads(f.read())\n",
    "    # Retrieve episodes\n",
    "    episodes = season['episodes']\n",
    "    # Iterate through the episodes\n",
    "    for episode in episodes:\n",
    "        # Retrieve scenes\n",
    "        scenes = episode['scenes']\n",
    "        # Iterate through the scenes\n",
    "        for scene in scenes:\n",
    "            # Retrieve utterances\n",
    "            utterances = scene['utterances']\n",
    "            # Iterate through the utterances\n",
    "            for utterance in utterances:\n",
    "                speaker = utterance['speakers']\n",
    "                \n",
    "                if len(speaker) == 1:\n",
    "                    try:\n",
    "                        #print('\\n\\nSpeaker:',speaker[0], \"Utter_id:\", utterance['utterance_id'], \":-\")\n",
    "                        \n",
    "                        tokens = utterance['tokens']\n",
    "                        speaker = utterance['speakers']\n",
    "                        character_entities = utterance['character_entities']\n",
    "\n",
    "                        #coz tokens is list of list where each list represents a sentence.\n",
    "                        for i in range(len(tokens)):\n",
    "                          \n",
    "                            #print(\"\\n\",tokens[i])\n",
    "                            \n",
    "                            #we associate \"None\" label to each token in character_entities\n",
    "                            target = ['None'] * len(tokens[i])\n",
    "                            cnt_None += len(target)\n",
    "                            \n",
    "                            \n",
    "                            \"\"\"\n",
    "                                \"character_entities\": [\n",
    "                                            [],\n",
    "                                            [[0, 1, \"Paul the Wine Guy\"], [4, 5, \"Paul the Wine Guy\"], [5, 6, \"Monica Geller\"]]\n",
    "                                ]\n",
    "                            \"\"\"\n",
    "                            \n",
    "                            #change target labels if character_entities[i] has some entries\n",
    "                            if character_entities[i]:\n",
    "                                for e in character_entities[i]:\n",
    "                                    \n",
    "                                    #start and end index for label\n",
    "                                    indexes = list(range(e[0], e[1]))\n",
    "                                    for j in indexes:\n",
    "                                        if e[2] in entities_dict:\n",
    "                                            target[j] = e[2]\n",
    "                                        else:\n",
    "                                            target[j] = 'Others'\n",
    "                                        \n",
    "                                        #subtracting None count as they are replaced\n",
    "                                        cnt_None -= 1\n",
    "                                        \n",
    "                                    \n",
    "                            # Insert data\n",
    "                            if speaker[0] in entities:\n",
    "                                data.append({'speaker': speaker[0], 'tokens': tokens[i]})\n",
    "                                labels.append(target)\n",
    "                            else:\n",
    "                                data.append({'speaker': 'Others', 'tokens': tokens[i]})\n",
    "                                labels.append(target)\n",
    "\n",
    "                            #print(\"\\nT:\",data[-1])\n",
    "                            #print(\"L:\",target)\n",
    "                            \n",
    "                            \n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "\n",
    "print(\"\\nTime to generate tokens: %.4f seconds.\\n\" % (timeit.default_timer() - start))\n",
    "\n",
    "print(\"data len:\", len(data))\n",
    "assert len(data) == len(labels), \"***Size Mismatch!!!***\"\n",
    "\n",
    "\n",
    "#adding count of None classes\n",
    "entities_dict['None'] = cnt_None\n",
    "entities_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda5.2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding present...\n",
      "Time to load: 58.7485 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "#25 dimensional word embeddings\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#If glove embeds is not in word2vec form then first convert it then load it\n",
    "if os.path.isfile('pretrained_embeds/gensim_glove_vectors.txt'):\n",
    "    glove_model = KeyedVectors.load_word2vec_format(\"pretrained_embeds/gensim_glove_vectors.txt\", binary=False)\n",
    "    print(\"Embedding present...\")\n",
    "    print(\"Time to load: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "\n",
    "    \n",
    "else:\n",
    "    glove2word2vec(glove_input_file=\"pretrained_embeds/glove.twitter.27B.25d.txt\", word2vec_output_file=\"pretrained_embeds/gensim_glove_vectors.txt\")\n",
    "    glove_model = KeyedVectors.load_word2vec_format(\"pretrained_embeds/gensim_glove_vectors.txt\", binary=False)\n",
    "    print(\"Time to build and load Word2Vec embeddings from Glove: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "\n",
    "    \n",
    "def get_embed(word):\n",
    "    # Case folding\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        return (glove_model.get_vector(word))\n",
    "    except:\n",
    "        return (glove_model.get_vector('unk'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41681\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#shuffling the data \n",
    "temp = list(zip(data, labels))\n",
    "random.shuffle(temp)\n",
    "data, labels = zip(*temp)\n",
    "\n",
    "data_embedding = []\n",
    "label_index = []\n",
    "\n",
    "#getting utterance embeddings \n",
    "for d in data:\n",
    "    temp = []\n",
    "    \n",
    "    # Getting embeddings for tokens \n",
    "    for t in d['tokens']:\n",
    "        temp.append(get_embed(t))\n",
    "    temp = np.asarray(temp)\n",
    "    \n",
    "    #gettting embeddings for speakers (We are taking average of first name and last name embeddings)\n",
    "    temp_speaker = []\n",
    "    for s in d['speaker'].split():\n",
    "        temp_speaker.append(get_embed(s))\n",
    "    \n",
    "    temp_speaker = np.asarray(temp_speaker)\n",
    "    temp_speaker = np.mean(temp_speaker, axis=0).reshape(1,-1)\n",
    "    \n",
    "    #concatanating speaker embedding with utterance embeddings before the token embeddings\n",
    "    temp = np.vstack((temp_speaker, temp))\n",
    "    data_embedding.append(temp)\n",
    "    \n",
    "    \n",
    "#getting label index from entities list\n",
    "for label in labels:\n",
    "    \n",
    "    #appending a None to compensate for speaker added in embeddings\n",
    "    label = ['None'] + label\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    for l in label:\n",
    "        temp.append(entities.index(l))\n",
    "   \n",
    "    temp = np.asarray(temp)\n",
    "    label_index.append(temp)\n",
    "    \n",
    "\n",
    "print(len(data_embedding))\n",
    "assert len(data_embedding) == len(label_index), \"***Size Mismatch!!!***\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data len: 33344\n",
      "Testing data len: 8337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRAIN_SIZE = int(len(data_embedding)*0.8)\n",
    "\n",
    "train_input = data_embedding[:TRAIN_SIZE]\n",
    "train_label_index = label_index[:TRAIN_SIZE]\n",
    "\n",
    "test_input = data_embedding[TRAIN_SIZE:]\n",
    "test_label_index = label_index[TRAIN_SIZE:]\n",
    "\n",
    "print(\"Training data len:\", len(train_input))\n",
    "assert len(train_input) == len(train_label_index), \"***Size Mismatch!!!***\"\n",
    "\n",
    "print(\"Testing data len:\", len(test_input))\n",
    "assert len(test_input) == len(test_label_index), \"***Size Mismatch!!!***\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Rachel Green\n",
      "1 Ross Geller\n",
      "2 Chandler Bing\n",
      "3 Monica Geller\n",
      "4 Joey Tribbiani\n",
      "5 Phoebe Buffay\n",
      "6 Others\n",
      "7 None\n",
      "\n",
      "\n",
      "['Dysprosium', '?']\n",
      "[7 7] \n",
      "\n",
      "['But', 'I', 'wo', \"n't\", '.']\n",
      "[7 5 7 7 7] \n",
      "\n",
      "['And', 'the', 'vet', 'said', 'it', 'was', 'time', '.']\n",
      "[7 7 7 7 6 7 7 7] \n",
      "\n",
      "['Oh', 'yeah', ',', 'but', 'do', \"n't\", 'worry', '.']\n",
      "[7 7 7 7 7 7 7 7] \n",
      "\n",
      "['I', 'know', '!']\n",
      "[0 7 7] \n",
      "\n",
      "['Okay', '.']\n",
      "[7 7] \n",
      "\n",
      "['Who', \"'s\", 'this', 'from', '?']\n",
      "[7 7 7 7 7] \n",
      "\n",
      "['Oh', 'yeah', ',', 'of', 'course', 'you', 'do', \"n't\", '!']\n",
      "[7 7 7 7 7 7 7 7 7] \n",
      "\n",
      "['So', 'you', 'did', \"n't\", 'leave', 'the', 'bank', '?']\n",
      "[7 1 7 7 7 7 7 7] \n",
      "\n",
      "['I', \"'m\", 'sorry', '!']\n",
      "[6 7 7 7] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,e in enumerate(entities):\n",
    "    print(i,e)\n",
    "\n",
    "print('\\n')\n",
    "for i in range(10):\n",
    "    print(data[i][\"tokens\"])\n",
    "    \n",
    "    #coz None is added in front to compensate for \n",
    "    print(train_label_index[i][1:], \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data\n",
    "\n",
    "data_path = 'dataV2/'\n",
    "\n",
    "#np.save() saves in  platform independent format\n",
    "#hot vector of entities labels\n",
    "np.save(data_path + 'entities-vector.npy', entities)\n",
    "\n",
    "#Train Input embeddings\n",
    "np.save(data_path + 'train_input.npy', train_input)\n",
    "\n",
    "#Test input embeddings \n",
    "np.save(data_path + 'test_input.npy', test_input)\n",
    "\n",
    "#Train label indexes\n",
    "np.save(data_path + 'train_label_index.npy', train_label_index)\n",
    "\n",
    "#Train label indexes\n",
    "np.save(data_path + 'test_label_index.npy', test_label_index)\n",
    "\n",
    "\n",
    "#text version of above binary files\n",
    "np.savetxt(data_path + 'entities-vector.txt', entities, fmt = \"%s\")\n",
    "\n",
    "np.savetxt(data_path + 'train_input.txt', train_input, fmt = \"%s\")\n",
    "np.savetxt(data_path + 'test_input.txt', test_input, \"%s\")\n",
    "\n",
    "np.savetxt(data_path + 'train_label_index.txt', train_label_index, fmt = \"%s\" )\n",
    "np.savetxt(data_path + 'test_label_index.txt', test_label_index, fmt = \"%s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
