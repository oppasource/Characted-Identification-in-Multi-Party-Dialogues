{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import re\n",
    "import string\n",
    "import os.path\n",
    "import timeit\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plot_graph\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Input, Dense, Embedding, Flatten, LSTM, Bidirectional, TimeDistributed, Dropout, Activation\n",
    "from keras.layers import LeakyReLU, concatenate\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "ip = open(\"dataV3/data_dump.txt\", \"rb\")\n",
    "dictionary = pickle.load(ip)\n",
    "ip.close()\n",
    "\n",
    "data = dictionary[\"data\"]\n",
    "labels = dictionary[\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 26402\n",
      "Total number of tokens: 279845\n",
      "Number of unique tokens: 10117\n",
      "Number of classes: 8\n",
      "\n",
      "frequency of each class\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None              228134\n",
       "Others             20937\n",
       "Ross Geller         7652\n",
       "Rachel Green        5454\n",
       "Chandler Bing       4815\n",
       "Joey Tribbiani      4685\n",
       "Monica Geller       4099\n",
       "Phoebe Buffay       4069\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all tokens\n",
    "words_all = list(chain.from_iterable([r[\"tokens\"] for r in data]))\n",
    "\n",
    "# unique tokens\n",
    "words = list(set(words_all))\n",
    "# will be used for padding\n",
    "words.append(\"ENDPAD\")\n",
    "\n",
    "VOCAB_SIZE = len(words)\n",
    "\n",
    "# to match the entity list used for previous versions\n",
    "\"\"\"\n",
    "0 Rachel Green\n",
    "1 Ross Geller\n",
    "2 Chandler Bing\n",
    "3 Monica Geller\n",
    "4 Joey Tribbiani\n",
    "5 Phoebe Buffay\n",
    "6 Others\n",
    "7 None\n",
    "\"\"\"\n",
    "\n",
    "# classes\n",
    "classes = [\"Rachel Green\", \"Ross Geller\", \"Chandler Bing\", \"Monica Geller\", \"Joey Tribbiani\", \n",
    "                 \"Phoebe Buffay\", \"Others\", \"None\"]\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(\"Number of sentences:\", len(data))\n",
    "print(\"Total number of tokens:\", len(words_all))\n",
    "print(\"Number of unique tokens:\", VOCAB_SIZE)\n",
    "print(\"Number of classes:\", NUM_CLASSES)\n",
    "\n",
    "\n",
    "print(\"\\nfrequency of each class\")\n",
    "pd.Series(list(chain.from_iterable(labels))).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using Speaker's embedding as a feature \n",
    "\n",
    "MAX_SENT_LEN = 25\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "class2idx = {t: i for i, t in enumerate(classes)}\n",
    "\n",
    "X = [[word2idx[w] for w in row[\"tokens\"]] for row in data]\n",
    "X_padded = pad_sequences(X, maxlen = MAX_SENT_LEN, padding = \"post\", value = word2idx['ENDPAD'], truncating = \"post\")\n",
    "\n",
    "Y = [[class2idx[l] for l in row] for row in labels]\n",
    "Y_padded = pad_sequences(Y, maxlen = MAX_SENT_LEN, padding=\"post\", value = class2idx[\"None\"], truncating = \"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10116 10117\n",
      "1 6 7\n",
      "\n",
      " W/o padding\n",
      "[9946, 6951, 779, 1770, 2826, 3114, 5275, 2046, 1626]\n",
      "[6, 7, 7, 7, 6, 3, 7, 7, 7]\n",
      "\n",
      " With padding\n",
      "[ 9946  6951   779  1770  2826  3114  5275  2046  1626 10116 10116 10116\n",
      " 10116 10116 10116 10116 10116 10116 10116 10116 10116 10116 10116 10116\n",
      " 10116]\n",
      "[6 7 7 7 6 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "print(word2idx['ENDPAD'], VOCAB_SIZE)\n",
    "print(class2idx['Ross Geller'], class2idx['Others'], class2idx['None'])\n",
    "\n",
    "print(\"\\n W/o padding\")\n",
    "print(X[0])\n",
    "print(Y[0])\n",
    "\n",
    "print(\"\\n With padding\")\n",
    "print(X_padded[0])\n",
    "print(Y_padded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6649  6335  6757  9663   779  3485  5487   681 10108 10116 10116 10116\n",
      " 10116 10116 10116 10116 10116 10116 10116 10116 10116 10116 10116 10116\n",
      " 10116]\n",
      "[2 7 7 2 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_padded, Y_padded, test_size = 0.2)\n",
    "\n",
    "# one hot encoding for Y\n",
    "y_train_ohe = np.array([to_categorical(i, num_classes= NUM_CLASSES) for i in y_train])\n",
    "y_test_ohe = np.array([to_categorical(i, num_classes= NUM_CLASSES) for i in y_test])\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(y_train_ohe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: <class 'numpy.ndarray'> (21121, 25)\n",
      "y_train: <class 'numpy.ndarray'> (21121, 25)\n",
      "y_train_ohe: <class 'numpy.ndarray'> (21121, 25, 8)\n",
      "\n",
      "x_test: <class 'numpy.ndarray'> (5281, 25)\n",
      "y_test: <class 'numpy.ndarray'> (5281, 25)\n",
      "y_test_ohe: <class 'numpy.ndarray'> (5281, 25, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train:\", type(x_train), x_train.shape)\n",
    "print(\"y_train:\", type(y_train), y_train.shape)\n",
    "print(\"y_train_ohe:\", type(y_train_ohe), y_train_ohe.shape)\n",
    "\n",
    "\n",
    "print(\"\\nx_test:\", type(x_test), x_test.shape)\n",
    "print(\"y_test:\", type(y_test), y_test.shape)\n",
    "print(\"y_test_ohe:\", type(y_test_ohe), y_test_ohe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 21121 samples, validate on 5281 samples\n",
      "Epoch 1/10\n",
      "21121/21121 [==============================] - 224s 11ms/step - loss: 0.1908 - acc: 0.9425 - val_loss: 0.1367 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.95038, saving model to modelsV3/val_acc-improvement-01-0.9504.hdf5\n",
      "Epoch 2/10\n",
      "21121/21121 [==============================] - 204s 10ms/step - loss: 0.1259 - acc: 0.9532 - val_loss: 0.1277 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.95038 to 0.95294, saving model to modelsV3/val_acc-improvement-02-0.9529.hdf5\n",
      "Epoch 3/10\n",
      "21121/21121 [==============================] - 207s 10ms/step - loss: 0.1176 - acc: 0.9557 - val_loss: 0.1235 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.95294 to 0.95393, saving model to modelsV3/val_acc-improvement-03-0.9539.hdf5\n",
      "Epoch 4/10\n",
      "21121/21121 [==============================] - 204s 10ms/step - loss: 0.1127 - acc: 0.9571 - val_loss: 0.1228 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.95393\n",
      "Epoch 5/10\n",
      "21121/21121 [==============================] - 200s 9ms/step - loss: 0.1078 - acc: 0.9590 - val_loss: 0.1240 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.95393\n",
      "Epoch 6/10\n",
      "21121/21121 [==============================] - 208s 10ms/step - loss: 0.1033 - acc: 0.9609 - val_loss: 0.1254 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.95393\n",
      "Epoch 7/10\n",
      "21121/21121 [==============================] - 220s 10ms/step - loss: 0.0991 - acc: 0.9625 - val_loss: 0.1281 - val_acc: 0.9529\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.95393\n",
      "Epoch 8/10\n",
      "21121/21121 [==============================] - 218s 10ms/step - loss: 0.0949 - acc: 0.9640 - val_loss: 0.1287 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.95393\n",
      "Epoch 9/10\n",
      "21121/21121 [==============================] - 214s 10ms/step - loss: 0.0912 - acc: 0.9658 - val_loss: 0.1316 - val_acc: 0.9533\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.95393\n",
      "Epoch 10/10\n",
      "21121/21121 [==============================] - 191s 9ms/step - loss: 0.0871 - acc: 0.9672 - val_loss: 0.1334 - val_acc: 0.9543\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.95393 to 0.95425, saving model to modelsV3/val_acc-improvement-10-0.9543.hdf5\n"
     ]
    }
   ],
   "source": [
    "encoded_data_path = \"modelsV3/\"\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "input = Input(shape=(MAX_SENT_LEN,))\n",
    "\n",
    "model = Embedding(input_dim = VOCAB_SIZE, output_dim = 50, input_length = MAX_SENT_LEN)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(NUM_CLASSES, activation=\"softmax\"))(model)  \n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# creating checkpoint to save model every time validation accuracy improves\n",
    "filepath = encoded_data_path + \"val_acc-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "history1 = model.fit(x_train, y_train_ohe,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x_test, y_test_ohe))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5281, 25) (5281, 25)\n",
      "(132025,) (132025,)\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.25      0.27      1103\n",
      "          1       0.34      0.41      0.38      1505\n",
      "          2       0.30      0.26      0.27       971\n",
      "          3       0.54      0.17      0.26       774\n",
      "          4       0.36      0.23      0.28       898\n",
      "          5       0.35      0.26      0.30       781\n",
      "          6       0.56      0.71      0.63      4230\n",
      "          7       1.00      1.00      1.00    121763\n",
      "\n",
      "avg / total       0.95      0.95      0.95    132025\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rachel Green</th>\n",
       "      <th>Ross Geller</th>\n",
       "      <th>Chandler Bing</th>\n",
       "      <th>Monica Geller</th>\n",
       "      <th>Joey Tribbiani</th>\n",
       "      <th>Phoebe Buffay</th>\n",
       "      <th>Others</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rachel Green</th>\n",
       "      <td>275</td>\n",
       "      <td>237</td>\n",
       "      <td>92</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>70</td>\n",
       "      <td>348</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ross Geller</th>\n",
       "      <td>134</td>\n",
       "      <td>623</td>\n",
       "      <td>91</td>\n",
       "      <td>16</td>\n",
       "      <td>75</td>\n",
       "      <td>63</td>\n",
       "      <td>472</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chandler Bing</th>\n",
       "      <td>90</td>\n",
       "      <td>149</td>\n",
       "      <td>249</td>\n",
       "      <td>21</td>\n",
       "      <td>61</td>\n",
       "      <td>47</td>\n",
       "      <td>334</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monica Geller</th>\n",
       "      <td>88</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "      <td>130</td>\n",
       "      <td>32</td>\n",
       "      <td>62</td>\n",
       "      <td>244</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joey Tribbiani</th>\n",
       "      <td>63</td>\n",
       "      <td>186</td>\n",
       "      <td>90</td>\n",
       "      <td>19</td>\n",
       "      <td>203</td>\n",
       "      <td>36</td>\n",
       "      <td>292</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoebe Buffay</th>\n",
       "      <td>98</td>\n",
       "      <td>123</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>202</td>\n",
       "      <td>246</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>172</td>\n",
       "      <td>359</td>\n",
       "      <td>147</td>\n",
       "      <td>25</td>\n",
       "      <td>111</td>\n",
       "      <td>91</td>\n",
       "      <td>2998</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>382</td>\n",
       "      <td>121305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Rachel Green  Ross Geller  Chandler Bing  Monica Geller  \\\n",
       "Rachel Green             275          237             92             11   \n",
       "Ross Geller              134          623             91             16   \n",
       "Chandler Bing             90          149            249             21   \n",
       "Monica Geller             88          112             99            130   \n",
       "Joey Tribbiani            63          186             90             19   \n",
       "Phoebe Buffay             98          123             63             16   \n",
       "Others                   172          359            147             25   \n",
       "None                      20           26             12              2   \n",
       "\n",
       "                Joey Tribbiani  Phoebe Buffay  Others    None  \n",
       "Rachel Green                47             70     348      23  \n",
       "Ross Geller                 75             63     472      31  \n",
       "Chandler Bing               61             47     334      20  \n",
       "Monica Geller               32             62     244       7  \n",
       "Joey Tribbiani             203             36     292       9  \n",
       "Phoebe Buffay               25            202     246       8  \n",
       "Others                     111             91    2998     327  \n",
       "None                         9              7     382  121305  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model= load_model(encoded_data_path +  \"weights-02-0.9058.hdf5\")\n",
    "\n",
    "y_predicted = model.predict(x_test)\n",
    "y_predicted = y_predicted.argmax(axis= 2)\n",
    "\n",
    "print(y_test.shape, y_predicted.shape)\n",
    "print(y_test.ravel().shape, y_predicted.ravel().shape)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test.ravel(), y_predicted.ravel())\n",
    "\n",
    "print ('\\nClassification Report:')\n",
    "print (classification_report(y_test.ravel(), y_predicted.ravel()))\n",
    "\n",
    "\n",
    "pd.DataFrame(confusion_mat, columns = classes, index = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "Train Loss",
         "type": "scatter",
         "uid": "eee9c506-f311-11e8-ae48-2089841a8cd2",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
         ],
         "y": [
          0.19084771395796352,
          0.12593117609313106,
          0.1176238184492559,
          0.11265821437362915,
          0.1078100690341155,
          0.10331024170855865,
          0.09912441404296866,
          0.0949420956184391,
          0.0912160758634777,
          0.08712189575048919
         ]
        },
        {
         "name": "Validation Loss",
         "type": "scatter",
         "uid": "eee9c507-f311-11e8-8287-2089841a8cd2",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
         ],
         "y": [
          0.13670390913475647,
          0.12767825158419788,
          0.12346303356464694,
          0.12276447754684124,
          0.1239876195808135,
          0.12535883123829292,
          0.12811802620097468,
          0.12867950454786,
          0.13162975516357198,
          0.13339205006096225
         ]
        }
       ],
       "layout": {
        "title": "Train Vs Validation Loss",
        "xaxis": {
         "dtick": 1,
         "tick0": 0,
         "ticklen": 8,
         "tickmode": "linear",
         "ticks": "outside",
         "tickwidth": 4
        },
        "yaxis": {
         "dtick": 1,
         "tick0": 0,
         "ticklen": 8,
         "tickmode": "linear",
         "ticks": "outside",
         "tickwidth": 4
        }
       }
      },
      "text/html": [
       "<div id=\"e8edbda9-e150-46ff-b9e1-c484b690c6d5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e8edbda9-e150-46ff-b9e1-c484b690c6d5\", [{\"name\": \"Train Loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], \"y\": [0.19084771395796352, 0.12593117609313106, 0.1176238184492559, 0.11265821437362915, 0.1078100690341155, 0.10331024170855865, 0.09912441404296866, 0.0949420956184391, 0.0912160758634777, 0.08712189575048919], \"type\": \"scatter\", \"uid\": \"eee9c506-f311-11e8-ae48-2089841a8cd2\"}, {\"name\": \"Validation Loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], \"y\": [0.13670390913475647, 0.12767825158419788, 0.12346303356464694, 0.12276447754684124, 0.1239876195808135, 0.12535883123829292, 0.12811802620097468, 0.12867950454786, 0.13162975516357198, 0.13339205006096225], \"type\": \"scatter\", \"uid\": \"eee9c507-f311-11e8-8287-2089841a8cd2\"}], {\"title\": \"Train Vs Validation Loss\", \"xaxis\": {\"dtick\": 1, \"tick0\": 0, \"ticklen\": 8, \"tickmode\": \"linear\", \"ticks\": \"outside\", \"tickwidth\": 4}, \"yaxis\": {\"dtick\": 1, \"tick0\": 0, \"ticklen\": 8, \"tickmode\": \"linear\", \"ticks\": \"outside\", \"tickwidth\": 4}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"e8edbda9-e150-46ff-b9e1-c484b690c6d5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e8edbda9-e150-46ff-b9e1-c484b690c6d5\", [{\"name\": \"Train Loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], \"y\": [0.19084771395796352, 0.12593117609313106, 0.1176238184492559, 0.11265821437362915, 0.1078100690341155, 0.10331024170855865, 0.09912441404296866, 0.0949420956184391, 0.0912160758634777, 0.08712189575048919], \"type\": \"scatter\", \"uid\": \"eee9c506-f311-11e8-ae48-2089841a8cd2\"}, {\"name\": \"Validation Loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], \"y\": [0.13670390913475647, 0.12767825158419788, 0.12346303356464694, 0.12276447754684124, 0.1239876195808135, 0.12535883123829292, 0.12811802620097468, 0.12867950454786, 0.13162975516357198, 0.13339205006096225], \"type\": \"scatter\", \"uid\": \"eee9c507-f311-11e8-8287-2089841a8cd2\"}], {\"title\": \"Train Vs Validation Loss\", \"xaxis\": {\"dtick\": 1, \"tick0\": 0, \"ticklen\": 8, \"tickmode\": \"linear\", \"ticks\": \"outside\", \"tickwidth\": 4}, \"yaxis\": {\"dtick\": 1, \"tick0\": 0, \"ticklen\": 8, \"tickmode\": \"linear\", \"ticks\": \"outside\", \"tickwidth\": 4}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = np.arange(0,15,1)\n",
    "y1 = history1.history[\"loss\"]\n",
    "name1 = \"Train Loss\"\n",
    "\n",
    "x2 = np.arange(0,15,1)\n",
    "y2 = history1.history[\"val_loss\"]\n",
    "name2 = \"Validation Loss\"\n",
    "\n",
    "plot_graph.trace(x1, y1, name1, x2, y2, name2, \"Train Vs Validation Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Speaker's embedding as a feature \n",
    "\n",
    "MAX_SENT_LEN2 = 25\n",
    "\n",
    "# to handle unknown words\n",
    "words.append(\"UNK\")\n",
    "\n",
    "word2idx2 = {w: i for i, w in enumerate(words + classes)}\n",
    "class2idx = {t: i for i, t in enumerate(classes)}\n",
    "\n",
    "# Adding Speaker at front\n",
    "X2 = [[word2idx2[row[\"speaker\"]]] + [word2idx[w] for w in row[\"tokens\"]] for row in data]\n",
    "X_padded2 = pad_sequences(X2, maxlen = MAX_SENT_LEN, padding = \"post\", value = word2idx['ENDPAD'], truncating = \"post\")\n",
    "\n",
    "# Adding None to compensate for adding Speaker at front\n",
    "Y2 = [[class2idx['None']] + [class2idx[l] for l in row] for row in labels]\n",
    "Y_padded2 = pad_sequences(Y2, maxlen = MAX_SENT_LEN, padding=\"post\", value = class2idx[\"None\"], truncating = \"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train2: <class 'numpy.ndarray'> (21121, 25)\n",
      "y_train2: <class 'numpy.ndarray'> (21121, 25)\n",
      "y_train_ohe2: <class 'numpy.ndarray'> (21121, 25, 8)\n",
      "\n",
      "x_test2: <class 'numpy.ndarray'> (5281, 25)\n",
      "y_test2: <class 'numpy.ndarray'> (5281, 25)\n",
      "y_test_ohe2: <class 'numpy.ndarray'> (5281, 25, 8)\n"
     ]
    }
   ],
   "source": [
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(X_padded2, Y_padded2, test_size = 0.2)\n",
    "\n",
    "# one hot encoding for Y\n",
    "y_train_ohe2 = np.array([to_categorical(i, num_classes= NUM_CLASSES) for i in y_train2])\n",
    "y_test_ohe2 = np.array([to_categorical(i, num_classes= NUM_CLASSES) for i in y_test2])\n",
    "\n",
    "print(\"x_train2:\", type(x_train2), x_train2.shape)\n",
    "print(\"y_train2:\", type(y_train2), y_train2.shape)\n",
    "print(\"y_train_ohe2:\", type(y_train_ohe2), y_train_ohe2.shape)\n",
    "\n",
    "\n",
    "print(\"\\nx_test2:\", type(x_test2), x_test2.shape)\n",
    "print(\"y_test2:\", type(y_test2), y_test2.shape)\n",
    "print(\"y_test_ohe2:\", type(y_test_ohe2), y_test_ohe2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10118  4175  7689  6335  9663  8055  4795  6335  9663  8055  3954  6713\n",
      "  8087 10116 10116 10116 10116 10116 10116 10116 10116 10116 10116 10116\n",
      " 10116]\n",
      "[7 7 7 7 0 7 7 7 0 7 1 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train2[1])\n",
    "print(y_train2[1])\n",
    "print(y_train_ohe2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 21121 samples, validate on 5281 samples\n",
      "Epoch 1/10\n",
      "21121/21121 [==============================] - 213s 10ms/step - loss: 0.1933 - acc: 0.9430 - val_loss: 0.1257 - val_acc: 0.9606\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96058, saving model to modelsV3/val_acc2-improvement-01-0.9606.hdf5\n",
      "Epoch 2/10\n",
      "21121/21121 [==============================] - 219s 10ms/step - loss: 0.0916 - acc: 0.9702 - val_loss: 0.0798 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96058 to 0.97313, saving model to modelsV3/val_acc2-improvement-02-0.9731.hdf5\n",
      "Epoch 3/10\n",
      "21121/21121 [==============================] - 238s 11ms/step - loss: 0.0719 - acc: 0.9753 - val_loss: 0.0767 - val_acc: 0.9738\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.97313 to 0.97379, saving model to modelsV3/val_acc2-improvement-03-0.9738.hdf5\n",
      "Epoch 4/10\n",
      "21121/21121 [==============================] - 220s 10ms/step - loss: 0.0672 - acc: 0.9764 - val_loss: 0.0733 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.97379 to 0.97474, saving model to modelsV3/val_acc2-improvement-04-0.9747.hdf5\n",
      "Epoch 5/10\n",
      "21121/21121 [==============================] - 222s 11ms/step - loss: 0.0601 - acc: 0.9785 - val_loss: 0.0733 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.97474\n",
      "Epoch 7/10\n",
      "21121/21121 [==============================] - 204s 10ms/step - loss: 0.0578 - acc: 0.9793 - val_loss: 0.0727 - val_acc: 0.9749\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.97474 to 0.97488, saving model to modelsV3/val_acc2-improvement-07-0.9749.hdf5\n",
      "Epoch 8/10\n",
      "21121/21121 [==============================] - 216s 10ms/step - loss: 0.0554 - acc: 0.9802 - val_loss: 0.0744 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.97488\n",
      "Epoch 9/10\n",
      "21121/21121 [==============================] - 210s 10ms/step - loss: 0.0529 - acc: 0.9810 - val_loss: 0.0755 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.97488\n",
      "Epoch 10/10\n",
      "21121/21121 [==============================] - 203s 10ms/step - loss: 0.0509 - acc: 0.9819 - val_loss: 0.0759 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.97488\n"
     ]
    }
   ],
   "source": [
    "encoded_data_path = \"modelsV3/\"\n",
    "\n",
    "VOCAB_SIZE2 = len(words + classes)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "input = Input(shape=(MAX_SENT_LEN2,))\n",
    "\n",
    "model = Embedding(input_dim = VOCAB_SIZE2, output_dim = 50, input_length = MAX_SENT_LEN2)(input)\n",
    "model = Dropout(0.2)(model)\n",
    "\n",
    "# each LSTM unit returns a sequence of output (of length same as i/p), one for each time step in the input data\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.2))(model)\n",
    "\n",
    "# It highlights that we intend to output NUM_CLASSES time step from the sequence for each time step in the input.\n",
    "# \n",
    "out = TimeDistributed(Dense(NUM_CLASSES, activation=\"softmax\"))(model)  \n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# creating checkpoint to save model every time validation accuracy improves\n",
    "filepath = encoded_data_path + \"val_acc2-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "history2 = model.fit(x_train2, y_train_ohe2,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x_test2, y_test_ohe2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5281, 25) (5281, 25)\n",
      "(132025,) (132025,)\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.67      0.71      1018\n",
      "          1       0.69      0.62      0.65      1541\n",
      "          2       0.85      0.66      0.74       954\n",
      "          3       0.93      0.66      0.77       773\n",
      "          4       0.69      0.63      0.66       964\n",
      "          5       0.79      0.70      0.74       803\n",
      "          6       0.66      0.83      0.74      4099\n",
      "          7       1.00      1.00      1.00    121873\n",
      "\n",
      "avg / total       0.98      0.97      0.97    132025\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rachel Green</th>\n",
       "      <th>Ross Geller</th>\n",
       "      <th>Chandler Bing</th>\n",
       "      <th>Monica Geller</th>\n",
       "      <th>Joey Tribbiani</th>\n",
       "      <th>Phoebe Buffay</th>\n",
       "      <th>Others</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rachel Green</th>\n",
       "      <td>683</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>229</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ross Geller</th>\n",
       "      <td>26</td>\n",
       "      <td>951</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>91</td>\n",
       "      <td>32</td>\n",
       "      <td>387</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chandler Bing</th>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>625</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>193</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monica Geller</th>\n",
       "      <td>29</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>508</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>137</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joey Tribbiani</th>\n",
       "      <td>18</td>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>606</td>\n",
       "      <td>25</td>\n",
       "      <td>219</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoebe Buffay</th>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>566</td>\n",
       "      <td>137</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>80</td>\n",
       "      <td>151</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>85</td>\n",
       "      <td>38</td>\n",
       "      <td>3420</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>442</td>\n",
       "      <td>121350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Rachel Green  Ross Geller  Chandler Bing  Monica Geller  \\\n",
       "Rachel Green             683           40             12              0   \n",
       "Ross Geller               26          951             20              6   \n",
       "Chandler Bing             32           50            625              4   \n",
       "Monica Geller             29           47              8            508   \n",
       "Joey Tribbiani            18           73              5              6   \n",
       "Phoebe Buffay             18           50              7              0   \n",
       "Others                    80          151             42             13   \n",
       "None                      19           17             14             11   \n",
       "\n",
       "                Joey Tribbiani  Phoebe Buffay  Others    None  \n",
       "Rachel Green                29             15     229      10  \n",
       "Ross Geller                 91             32     387      28  \n",
       "Chandler Bing               16             16     193      18  \n",
       "Monica Geller               25             16     137       3  \n",
       "Joey Tribbiani             606             25     219      12  \n",
       "Phoebe Buffay               15            566     137      10  \n",
       "Others                      85             38    3420     270  \n",
       "None                        10             10     442  121350  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= load_model(\"modelsV3/val_acc2-improvement-07-0.9749.hdf5\")\n",
    "\n",
    "y_predicted = model.predict(x_test2)\n",
    "y_predicted = y_predicted.argmax(axis= 2)\n",
    "\n",
    "print(y_test2.shape, y_predicted.shape)\n",
    "print(y_test2.ravel().shape, y_predicted.ravel().shape)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test2.ravel(), y_predicted.ravel())\n",
    "\n",
    "print ('\\nClassification Report:')\n",
    "print (classification_report(y_test2.ravel(), y_predicted.ravel()))\n",
    "\n",
    "pd.DataFrame(confusion_mat, columns = classes, index = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "Train Loss",
         "type": "scatter",
         "uid": "c2ddd24c-f311-11e8-af21-2089841a8cd2",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "y": [
          0.19328565535956918,
          0.09163184982460053,
          0.07192462968316066,
          0.06716290074437864,
          0.06340482325817982,
          0.0601400130269935,
          0.057794122561825804,
          0.05541605029713427,
          0.05286846980367876,
          0.050858977779317104
         ]
        },
        {
         "name": "Validation Loss",
         "type": "scatter",
         "uid": "c2ddd24d-f311-11e8-b47c-2089841a8cd2",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "y": [
          0.12566364582933456,
          0.07975039319476493,
          0.0766692374115346,
          0.07334125087108154,
          0.07364495191423308,
          0.07332551240627569,
          0.07270863596924868,
          0.07436711690322487,
          0.0755386876012538,
          0.07592197424804277
         ]
        }
       ],
       "layout": {
        "title": "Train Vs Validation Loss",
        "xaxis": {
         "dtick": 1,
         "tick0": 0,
         "ticklen": 8,
         "tickmode": "linear",
         "ticks": "outside",
         "tickwidth": 4
        },
        "yaxis": {
         "dtick": 1,
         "tick0": 0,
         "ticklen": 8,
         "tickmode": "linear",
         "ticks": "outside",
         "tickwidth": 4
        }
       }
      },
      "text/html": [
       "<div id=\"79d12a25-7612-4bd7-9f51-a6068e7ceb8d\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"79d12a25-7612-4bd7-9f51-a6068e7ceb8d\", [{\"name\": \"Train Loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [0.19328565535956918, 0.09163184982460053, 0.07192462968316066, 0.06716290074437864, 0.06340482325817982, 0.0601400130269935, 0.057794122561825804, 0.05541605029713427, 0.05286846980367876, 0.050858977779317104], \"type\": \"scatter\", \"uid\": \"c2ddd24c-f311-11e8-af21-2089841a8cd2\"}, {\"name\": \"Validation Loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [0.12566364582933456, 0.07975039319476493, 0.0766692374115346, 0.07334125087108154, 0.07364495191423308, 0.07332551240627569, 0.07270863596924868, 0.07436711690322487, 0.0755386876012538, 0.07592197424804277], \"type\": \"scatter\", \"uid\": \"c2ddd24d-f311-11e8-b47c-2089841a8cd2\"}], {\"title\": \"Train Vs Validation Loss\", \"xaxis\": {\"dtick\": 1, \"tick0\": 0, \"ticklen\": 8, \"tickmode\": \"linear\", \"ticks\": \"outside\", \"tickwidth\": 4}, \"yaxis\": {\"dtick\": 1, \"tick0\": 0, \"ticklen\": 8, \"tickmode\": \"linear\", \"ticks\": \"outside\", \"tickwidth\": 4}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"79d12a25-7612-4bd7-9f51-a6068e7ceb8d\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"79d12a25-7612-4bd7-9f51-a6068e7ceb8d\", [{\"name\": \"Train Loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [0.19328565535956918, 0.09163184982460053, 0.07192462968316066, 0.06716290074437864, 0.06340482325817982, 0.0601400130269935, 0.057794122561825804, 0.05541605029713427, 0.05286846980367876, 0.050858977779317104], \"type\": \"scatter\", \"uid\": \"c2ddd24c-f311-11e8-af21-2089841a8cd2\"}, {\"name\": \"Validation Loss\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [0.12566364582933456, 0.07975039319476493, 0.0766692374115346, 0.07334125087108154, 0.07364495191423308, 0.07332551240627569, 0.07270863596924868, 0.07436711690322487, 0.0755386876012538, 0.07592197424804277], \"type\": \"scatter\", \"uid\": \"c2ddd24d-f311-11e8-b47c-2089841a8cd2\"}], {\"title\": \"Train Vs Validation Loss\", \"xaxis\": {\"dtick\": 1, \"tick0\": 0, \"ticklen\": 8, \"tickmode\": \"linear\", \"ticks\": \"outside\", \"tickwidth\": 4}, \"yaxis\": {\"dtick\": 1, \"tick0\": 0, \"ticklen\": 8, \"tickmode\": \"linear\", \"ticks\": \"outside\", \"tickwidth\": 4}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = np.arange(0,11,1)\n",
    "y1 = history2.history[\"loss\"]\n",
    "name1 = \"Train Loss\"\n",
    "\n",
    "x2 = np.arange(0,11,1)\n",
    "y2 = history2.history[\"val_loss\"]\n",
    "name2 = \"Validation Loss\"\n",
    "\n",
    "plot_graph.trace(x1, y1, name1, x2, y2, name2, \"Train Vs Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 21121 samples, validate on 5281 samples\n",
      "Epoch 1/10\n",
      "21121/21121 [==============================] - 150s 7ms/step - loss: 0.2148 - acc: 0.9424 - val_loss: 0.1292 - val_acc: 0.9598\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.95977, saving model to modelsV3/val_acc3-improvement-01-0.9598.hdf5\n",
      "Epoch 2/10\n",
      "21121/21121 [==============================] - 132s 6ms/step - loss: 0.1038 - acc: 0.9673 - val_loss: 0.0860 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.95977 to 0.97253, saving model to modelsV3/val_acc3-improvement-02-0.9725.hdf5\n",
      "Epoch 3/10\n",
      "21121/21121 [==============================] - 142s 7ms/step - loss: 0.0778 - acc: 0.9741 - val_loss: 0.0785 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.97253 to 0.97341, saving model to modelsV3/val_acc3-improvement-03-0.9734.hdf5\n",
      "Epoch 4/10\n",
      "21121/21121 [==============================] - 136s 6ms/step - loss: 0.0721 - acc: 0.9753 - val_loss: 0.0764 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.97341\n",
      "Epoch 5/10\n",
      "21121/21121 [==============================] - 143s 7ms/step - loss: 0.0687 - acc: 0.9761 - val_loss: 0.0750 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.97341\n",
      "Epoch 6/10\n",
      "21121/21121 [==============================] - 140s 7ms/step - loss: 0.0663 - acc: 0.9767 - val_loss: 0.0743 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.97341 to 0.97357, saving model to modelsV3/val_acc3-improvement-06-0.9736.hdf5\n",
      "Epoch 7/10\n",
      "21121/21121 [==============================] - 127s 6ms/step - loss: 0.0640 - acc: 0.9772 - val_loss: 0.0746 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.97357\n",
      "Epoch 8/10\n",
      "21121/21121 [==============================] - 126s 6ms/step - loss: 0.0629 - acc: 0.9776 - val_loss: 0.0756 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.97357\n",
      "Epoch 9/10\n",
      "21121/21121 [==============================] - 130s 6ms/step - loss: 0.0609 - acc: 0.9782 - val_loss: 0.0750 - val_acc: 0.9735\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.97357\n",
      "Epoch 10/10\n",
      "21121/21121 [==============================] - 136s 6ms/step - loss: 0.0594 - acc: 0.9786 - val_loss: 0.0759 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.97357\n"
     ]
    }
   ],
   "source": [
    "encoded_data_path = \"modelsV3/\"\n",
    "\n",
    "VOCAB_SIZE2 = len(words + classes)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "input = Input(shape=(MAX_SENT_LEN2,))\n",
    "\n",
    "model = Embedding(input_dim = VOCAB_SIZE2, output_dim = 50, input_length = MAX_SENT_LEN2)(input)\n",
    "model = Dropout(0.2)(model)\n",
    "\n",
    "# each LSTM unit returns a sequence of output (of length same as i/p), one for each time step in the input data\n",
    "model = LSTM(units=100, return_sequences=True, recurrent_dropout=0.2, activation='relu')(model)\n",
    "\n",
    "model = TimeDistributed(Dense(32, activation=\"relu\"))(model) \n",
    "\n",
    "out = TimeDistributed(Dense(NUM_CLASSES, activation=\"softmax\"))(model)  \n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# creating checkpoint to save model every time validation accuracy improves\n",
    "filepath = encoded_data_path + \"val_acc3-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "history3 = model.fit(x_train2, y_train_ohe2,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x_test2, y_test_ohe2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5281, 25) (5281, 25)\n",
      "(132025,) (132025,)\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.67      0.71      1096\n",
      "          1       0.73      0.65      0.69      1454\n",
      "          2       0.91      0.60      0.72       959\n",
      "          3       0.96      0.69      0.80       792\n",
      "          4       0.70      0.63      0.67       947\n",
      "          5       0.89      0.66      0.76       741\n",
      "          6       0.61      0.87      0.72      4101\n",
      "          7       1.00      0.99      1.00    121935\n",
      "\n",
      "avg / total       0.98      0.97      0.97    132025\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rachel Green</th>\n",
       "      <th>Ross Geller</th>\n",
       "      <th>Chandler Bing</th>\n",
       "      <th>Monica Geller</th>\n",
       "      <th>Joey Tribbiani</th>\n",
       "      <th>Phoebe Buffay</th>\n",
       "      <th>Others</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rachel Green</th>\n",
       "      <td>731</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>280</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ross Geller</th>\n",
       "      <td>15</td>\n",
       "      <td>945</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>368</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chandler Bing</th>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>578</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>278</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monica Geller</th>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>547</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>153</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joey Tribbiani</th>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>599</td>\n",
       "      <td>4</td>\n",
       "      <td>254</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoebe Buffay</th>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>491</td>\n",
       "      <td>165</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Others</th>\n",
       "      <td>92</td>\n",
       "      <td>130</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>102</td>\n",
       "      <td>10</td>\n",
       "      <td>3556</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>764</td>\n",
       "      <td>121088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Rachel Green  Ross Geller  Chandler Bing  Monica Geller  \\\n",
       "Rachel Green             731           43              6              1   \n",
       "Ross Geller               15          945             14              1   \n",
       "Chandler Bing             33           32            578              2   \n",
       "Monica Geller             17           42              5            547   \n",
       "Joey Tribbiani            35           38              2              0   \n",
       "Phoebe Buffay             19           41              6              0   \n",
       "Others                    92          130             23             10   \n",
       "None                      21           15              4              8   \n",
       "\n",
       "                Joey Tribbiani  Phoebe Buffay  Others    None  \n",
       "Rachel Green                16              6     280      13  \n",
       "Ross Geller                 71              9     368      31  \n",
       "Chandler Bing               14             12     278      10  \n",
       "Monica Geller               19              6     153       3  \n",
       "Joey Tribbiani             599              4     254      15  \n",
       "Phoebe Buffay               12            491     165       7  \n",
       "Others                     102             10    3556     178  \n",
       "None                        19             16     764  121088  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= load_model(\"modelsV3/val_acc3-improvement-06-0.9736.hdf5\")\n",
    "\n",
    "y_predicted = model.predict(x_test2)\n",
    "y_predicted = y_predicted.argmax(axis= 2)\n",
    "\n",
    "print(y_test2.shape, y_predicted.shape)\n",
    "print(y_test2.ravel().shape, y_predicted.ravel().shape)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test2.ravel(), y_predicted.ravel())\n",
    "\n",
    "print ('\\nClassification Report:')\n",
    "print (classification_report(y_test2.ravel(), y_predicted.ravel()))\n",
    "\n",
    "pd.DataFrame(confusion_mat, columns = classes, index = classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
