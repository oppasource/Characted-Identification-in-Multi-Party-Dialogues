{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import Stemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import timeit\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm\n",
    "\n",
    "\n",
    "def build_data(train_input_, train_label_index):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(train_input_)):\n",
    "        for j in range(len(train_input_[i])):\n",
    "            #x.append(normalize(train_input_[i][j]))\n",
    "            x.append(train_input_[i][j])\n",
    "            y.append(train_label_index[i][j])\n",
    "                    \n",
    "    return (np.array(x), np.array(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded!!!\n",
      "\n",
      "Train data len: 73447\n",
      "Train data len: 17288\n",
      "\n",
      "Data formatting done!!!\n",
      "\n",
      "x_train shape (73447, 25)\n",
      "y_train shape (73447,)\n",
      "y_train_one_hot shape (73447, 8)\n",
      "\n",
      "x_test shape (17288, 25)\n",
      "y_test shape (17288,)\n",
      "y_test_one_hot shape (17288, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = 'dataV2-Small/'\n",
    "TRAIN_SIZE = 10000\n",
    "TEST_SIZE = int(TRAIN_SIZE*.2)\n",
    "\n",
    "#Load Embeddings\n",
    "    \n",
    "#Train data embeddings\n",
    "train_input_ = np.load(data_path + 'train_input.npy')\n",
    "\n",
    "#Train labels as indexes\n",
    "train_label_index_ = np.load(data_path + 'train_label_index.npy')\n",
    "\n",
    "\n",
    "#Test data embeddings\n",
    "test_input_ = np.load(data_path + 'test_input.npy')\n",
    "\n",
    "#Test labels as indexes\n",
    "test_label_index_ = np.load(data_path + 'test_label_index.npy')\n",
    "\n",
    "\n",
    "print(\"Data Loaded!!!\\n\")\n",
    "\n",
    "x_train, y_train = build_data(train_input_[:TRAIN_SIZE], train_label_index_[:TRAIN_SIZE])\n",
    "\n",
    "print(\"Train data len:\", len(x_train))\n",
    "assert len(x_train) == len(y_train), \"**Size Mismatch!!!***\"\n",
    "\n",
    "\n",
    "x_test, y_test = build_data(test_input_[:TEST_SIZE], test_label_index_[:TEST_SIZE])\n",
    "\n",
    "print(\"Train data len:\", len(x_test))\n",
    "assert len(x_test) == len(y_test), \"**Size Mismatch!!!***\"\n",
    "\n",
    "print(\"\\nData formatting done!!!\")\n",
    "\n",
    "\n",
    "# transform labels into one hot representation\n",
    "y_train_one_hot = (np.arange(np.max(y_train) + 1) == y_train[:, None]).astype(float)\n",
    "\n",
    "y_test_one_hot = (np.arange(np.max(y_test) + 1) == y_test[:, None]).astype(float)\n",
    "\n",
    "#lr = np.arange(OP_DIM)\n",
    "#test_labels_one_hot = (lr==test_labels).astype(np.float)\n",
    "\n",
    "\"\"\"\n",
    "#removing zeroes and ones from the labels:\n",
    "y_train_one_hot[y_train_one_hot==0] = 0.01\n",
    "y_train_one_hot[y_train_one_hot==1] = 0.99\n",
    "\n",
    "y_test_one_hot[y_test_one_hot==0] = 0.01\n",
    "y_test_one_hot[y_test_one_hot==1] = 0.99\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nx_train shape\", x_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"y_train_one_hot shape\", y_train_one_hot.shape)\n",
    "\n",
    "\n",
    "print(\"\\nx_test shape\", x_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)\n",
    "print(\"y_test_one_hot shape\", y_test_one_hot.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5000, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy',])\n",
    "\n",
    "model.fit(X_train, y_train,epochs=5,batch_size=2000)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, batch_size=2000)\n",
    "score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train data len: 73447\n",
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 73447 samples, validate on 17288 samples\n",
      "Epoch 1/5\n",
      "73447/73447 [==============================] - 5s 74us/step - loss: 0.4055 - acc: 0.8829 - val_loss: 0.2965 - val_acc: 0.9072\n",
      "Epoch 2/5\n",
      "73447/73447 [==============================] - 5s 67us/step - loss: 0.3045 - acc: 0.9021 - val_loss: 0.2683 - val_acc: 0.9111\n",
      "Epoch 3/5\n",
      "73447/73447 [==============================] - 5s 67us/step - loss: 0.2831 - acc: 0.9064 - val_loss: 0.2571 - val_acc: 0.9140\n",
      "Epoch 4/5\n",
      "73447/73447 [==============================] - 5s 66us/step - loss: 0.2750 - acc: 0.9073 - val_loss: 0.2491 - val_acc: 0.9159\n",
      "Epoch 5/5\n",
      "73447/73447 [==============================] - 5s 69us/step - loss: 0.2703 - acc: 0.9086 - val_loss: 0.2489 - val_acc: 0.9137\n",
      "\n",
      "\n",
      "Total training time: 25.8022 seconds.\n",
      "17288/17288 [==============================] - 0s 21us/step\n",
      "\n",
      "Testing time: 0.3590 seconds.\n",
      "\n",
      "Test score: 0.24893969592536175\n",
      "Test accuracy: 0.9136973623322536\n"
     ]
    }
   ],
   "source": [
    "#MLP Network architecture\n",
    "\n",
    "IP_DIM = 25 #x_train.shape[1]\n",
    "OP_DIM = 8  #y_train_one_hot.shape[1]\n",
    "\n",
    "\n",
    "print('Building model...')\n",
    "model_mlp = Sequential()\n",
    "\n",
    "#max_features = 20000 #size of embedding\n",
    "#Embedding(input_dim, output_dim, embeddings_initializer='uniform', ***, input_length=None)\n",
    "#o/p will be model.output_shape == (None, 10 :input_dim, 64:output_dim), where None is the batch dimension of the matrix given.\n",
    "#model_mlp.add(Embedding(max_features, 100, input_length=50))\n",
    "\n",
    "## Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape: here, 20-dimensional vectors.\n",
    "model_mlp.add(Dense(100, input_dim= IP_DIM, activation='relu'))\n",
    "model_mlp.add(Dropout(0.5))\n",
    "\n",
    "model_mlp.add(Dense(50, activation='relu'))\n",
    "model_mlp.add(Dropout(0.5))\n",
    "\n",
    "model_mlp.add(Dense(OP_DIM, activation='softmax'))\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_mlp.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  #optimizer = sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(\"Train data len:\", len(x_train))\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#batch_size: Integer or None. Number of samples per gradient update. \n",
    "#If unspecified, batch_size will default to 32.\n",
    "model_mlp.fit(x_train, y_train_one_hot,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              validation_data=(x_test, y_test_one_hot))\n",
    "\n",
    "#model_lstm.fit(data, np.array(labels), validation_split=0.2, epochs=3)\n",
    "\n",
    "print(\"\\n\\nTotal training time: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "score, acc = model_mlp.evaluate(x_test, y_test_one_hot, batch_size = BATCH_SIZE)\n",
    "\n",
    "print(\"\\nTesting time: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "print('\\nTest score:', score)\n",
    "\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "model.predict(x, batch_size=None, verbose=0, steps=None)\n",
    "Generates output predictions for the input samples.\n",
    "\n",
    "Computation is done in batches.\n",
    "\n",
    "Arguments\n",
    "\n",
    "x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n",
    "\n",
    "batch_size: Integer. If unspecified, it will default to 32.\n",
    "\n",
    "verbose: Verbosity mode, 0 or 1.\n",
    "\n",
    "steps: Total number of steps (batches of samples) before declaring the prediction round finished. \n",
    "Ignored with the default value of None.\n",
    "\n",
    "Returns: Numpy array(s) of predictions.\n",
    "\n",
    "Raises\n",
    "\n",
    "ValueError: In case of mismatch between the provided input data and the model's expectations, \n",
    "or in case a stateful model receives a number of samples that is not a multiple of the batch size.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sample = process_sample([\"Is this is a fucking joke?\", \"This is good.\", \"This is insane man!!!\"])\n",
    "\n",
    "print(model_lstm.predict_classes(sample))\n",
    "\n",
    "print(model_mlp.predict_classes(sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 50, 1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train data len: 19826 , Test data len: 4957\n",
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 19826 samples, validate on 4957 samples\n",
      "Epoch 1/3\n",
      "19826/19826 [==============================] - 78s 4ms/step - loss: 0.1769 - acc: 0.9279 - val_loss: 0.3585 - val_acc: 0.8572\n",
      "Epoch 2/3\n",
      "19826/19826 [==============================] - 87s 4ms/step - loss: 0.1051 - acc: 0.9602 - val_loss: 0.3682 - val_acc: 0.8439\n",
      "Epoch 3/3\n",
      "19826/19826 [==============================] - 87s 4ms/step - loss: 0.0953 - acc: 0.9640 - val_loss: 0.4175 - val_acc: 0.8449\n",
      "\n",
      "\n",
      "Total training time: 254.2815 seconds.\n",
      "4957/4957 [==============================] - 5s 951us/step\n",
      "\n",
      "Testing time: 4.7194 seconds.\n",
      "\n",
      "Test score: 0.41748238686642775\n",
      "Test accuracy: 0.8448658461938203\n"
     ]
    }
   ],
   "source": [
    "#LSTM Network architecture\n",
    "\n",
    "IP_DIM = 25 #x_train.shape[1]\n",
    "OP_DIM = 8  #y_train_one_hot.shape[1]\n",
    "\n",
    "print('Building model...')\n",
    "\n",
    "#The network starts with an embedding layer.\n",
    "#Turns positive integers (indexes) into dense vectors of fixed size allowing the n/w to represent a word in a meaningful way.\n",
    "#eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "#This layer can only be used as the first layer in a model.\n",
    "\n",
    "#keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', ***, input_length=None)\n",
    "\n",
    "#input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "\n",
    "#output_dim: int >= 0. Dimension of the dense embedding.\n",
    "\n",
    "#input_length: Length of input sequences, when it is constant. \n",
    "#This argument is required if you are going to connect Flatten then Dense layers upstream \n",
    "#(without it, the shape of the dense outputs cannot be computed).\n",
    "\n",
    "#eg. model.add(Embedding(1000, 64, input_length=10))\n",
    "\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# where the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "\n",
    "# o/p will be model.output_shape == (None, 10 :input_dim, 64:output_dim), where None is the batch dimension of the matrix given.\n",
    "\n",
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.add(Embedding(20000, 100, input_length=IP_DIM))\n",
    "\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model_lstm.compile(loss='categorical_crossentropy', \n",
    "                   optimizer='rmsprop', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "print(\"Train data len:\", len(X_train), \", Test data len:\", len(X_test))\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#batch_size: Integer or None. Number of samples per gradient update. \n",
    "#If unspecified, batch_size will default to 32.\n",
    "model_lstm.fit(x_train_seq, y_train_one_hot,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              validation_data=(x_test, y_test_one_hot))\n",
    "\n",
    "#model_lstm.fit(data, np.array(labels), validation_split=0.2, epochs=3)\n",
    "\n",
    "print(\"\\n\\nTotal training time: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "score, acc = model_lstm.evaluate(x_test_seq, y_test, batch_size = BATCH_SIZE)\n",
    "\n",
    "print(\"\\nTesting time: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "print('\\nTest score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
