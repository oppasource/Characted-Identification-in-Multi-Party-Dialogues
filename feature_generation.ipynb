{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rachel Green',\n",
       " 'Ross Geller',\n",
       " 'Chandler Bing',\n",
       " 'Monica Geller',\n",
       " 'Joey Tribbiani',\n",
       " 'Phoebe Buffay',\n",
       " 'Others',\n",
       " 'None']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "characters = {}\n",
    "\n",
    "for i in range(1,11):\n",
    "    # Each seasons file path\n",
    "    f = open('json_data/friends_season_'+ str(i).zfill(2) +'.json', 'r')\n",
    "    # Loading seaosns JSON\n",
    "    season = json.loads(f.read())\n",
    "    # Retrieve episodes\n",
    "    episodes = season['episodes']\n",
    "    # Iterate through the episodes\n",
    "    for episode in episodes:\n",
    "        # Retrieve scenes\n",
    "        scenes = episode['scenes']\n",
    "        # Iterate through the scenes\n",
    "        for scene in scenes:\n",
    "            # Retrieve utterances\n",
    "            utterances = scene['utterances']\n",
    "            # Iterate through the utterances\n",
    "            for utterance in utterances:\n",
    "                speaker = utterance['speakers']\n",
    "                if len(speaker) == 1:\n",
    "                    speaker = speaker[0]\n",
    "                    characters[speaker] = characters.get(speaker, 0) + 1\n",
    "\n",
    "# We will use characters who have spoken the most utterances and remaining will be considered in 'others' class\n",
    "entities = [i[0] for i in sorted(characters.items(), key = lambda kv: kv[1], reverse=True)[:6]]\n",
    "# Thus we will add two more classes 'Others' and 'None'\n",
    "entities += ['Others', 'None']\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21121\n",
      "21121\n",
      "5281\n",
      "5281\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []        \n",
    "        \n",
    "# Sepearte out data and labels\n",
    "for i in range(1,11):\n",
    "    # Each seasons file path\n",
    "    f = open('json_data/friends_season_'+ str(i).zfill(2) +'.json', 'r')\n",
    "    # Loading seaosns JSON\n",
    "    season = json.loads(f.read())\n",
    "    # Retrieve episodes\n",
    "    episodes = season['episodes']\n",
    "    # Iterate through the episodes\n",
    "    for episode in episodes:\n",
    "        # Retrieve scenes\n",
    "        scenes = episode['scenes']\n",
    "        # Iterate through the scenes\n",
    "        for scene in scenes:\n",
    "            # Retrieve utterances\n",
    "            utterances = scene['utterances']\n",
    "            # Iterate through the utterances\n",
    "            for utterance in utterances:\n",
    "                speaker = utterance['speakers']\n",
    "                if len(speaker) == 1:\n",
    "                    try:\n",
    "                        tokens = utterance['tokens']\n",
    "                        speaker = utterance['speakers']\n",
    "                        character_entities = utterance['character_entities']\n",
    "\n",
    "\n",
    "                        for i in range(len(tokens)):\n",
    "                            if character_entities[i]:\n",
    "                                target = ['None'] * len(tokens[i])\n",
    "                                for e in character_entities[i]:\n",
    "                                    indexes = list(range(e[0], e[1]))\n",
    "                                    for j in indexes:\n",
    "                                        if e[2] in entities:\n",
    "                                            target[j] = e[2]\n",
    "                                        else:\n",
    "                                            target[j] = 'Others'\n",
    "\n",
    "                                # Insert data\n",
    "                                if speaker[0] in entities:\n",
    "                                    data.append({'speaker': speaker[0], 'tokens': tokens[i]})\n",
    "                                    labels.append(target)\n",
    "                                else:\n",
    "                                    data.append({'speaker': 'Others', 'tokens': tokens[i]})\n",
    "                                    labels.append(target)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "# Shuffling the data \n",
    "temp = list(zip(data, labels))\n",
    "random.shuffle(temp)\n",
    "data, labels = zip(*temp)\n",
    "\n",
    "train_thresh = int(len(data)*0.8)\n",
    "\n",
    "train_data = data[:train_thresh]\n",
    "train_labels = labels[:train_thresh]\n",
    "\n",
    "test_data = data[train_thresh:]\n",
    "test_labels = labels[train_thresh:]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(train_labels))\n",
    "print(len(test_data))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# Create a directory 'pretrained_embeds/' in the same directory as this notebook\n",
    "# Download twitter embeddings from http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "# Unzip it and place file 'glove.twitter.27B.25d.txt' in 'pretrained_embeds/' directory.\n",
    "\n",
    "# We are doing it with 25 dimensional word embeddings, however we can try doing with more \n",
    "# dimensional embeddings available.\n",
    "\n",
    "# If glove embeds is not in word2vec form then first convert it then load it\n",
    "if os.path.isfile('pretrained_embeds/gensim_glove_vectors.txt'):\n",
    "    glove_model = KeyedVectors.load_word2vec_format(\"pretrained_embeds/gensim_glove_vectors.txt\", binary=False)\n",
    "else:\n",
    "    glove2word2vec(glove_input_file=\"pretrained_embeds/glove.twitter.27B.25d.txt\", word2vec_output_file=\"pretrained_embeds/gensim_glove_vectors.txt\")\n",
    "    glove_model = KeyedVectors.load_word2vec_format(\"pretrained_embeds/gensim_glove_vectors.txt\", binary=False)\n",
    "\n",
    "def get_embed(word):\n",
    "    # Case folding\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        return (glove_model.get_vector(word))\n",
    "    except:\n",
    "        return (glove_model.get_vector('unk'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21121\n",
      "21121\n",
      "5281\n"
     ]
    }
   ],
   "source": [
    "train_input = []\n",
    "train_one_hot = []\n",
    "\n",
    "test_input = []\n",
    "\n",
    "# Loop to get embeddings in train_input \n",
    "for d in train_data:\n",
    "    temp = []\n",
    "    \n",
    "    # Getting embeddings for tokens \n",
    "    for t in d['tokens']:\n",
    "        temp.append(get_embed(t))\n",
    "    temp = np.asarray(temp)\n",
    "    \n",
    "    # Gettting embeddings for speakers (We are taking average of first name and last name embeddings)\n",
    "    temp_speaker = []\n",
    "    for s in d['speaker'].split():\n",
    "        temp_speaker.append(get_embed(s))\n",
    "    temp_speaker = np.asarray(temp_speaker)\n",
    "    temp_speaker = np.mean(temp_speaker, axis=0).reshape(1,-1)\n",
    "    \n",
    "    # Concatinating speaker embedding with utterance embeddings\n",
    "    temp = np.vstack((temp_speaker, temp))\n",
    "    train_input.append(temp)\n",
    "    \n",
    "    \n",
    "# Loop to get embeddings in test_input\n",
    "for d in test_data:\n",
    "    temp = []\n",
    "    \n",
    "    # Getting embeddings for tokens \n",
    "    for t in d['tokens']:\n",
    "        temp.append(get_embed(t))\n",
    "    temp = np.asarray(temp)\n",
    "    \n",
    "    # Gettting embeddings for speakers (We are taking average of first name and last name embeddings)\n",
    "    temp_speaker = []\n",
    "    for s in d['speaker'].split():\n",
    "        temp_speaker.append(get_embed(s))\n",
    "    temp_speaker = np.asarray(temp_speaker)\n",
    "    temp_speaker = np.mean(temp_speaker, axis=0).reshape(1,-1)\n",
    "    \n",
    "    # Concatinating speaker embedding with utterance embeddings\n",
    "    temp = np.vstack((temp_speaker, temp))\n",
    "    test_input.append(temp)\n",
    "    \n",
    "    \n",
    "# Loop to get embeddings in test_input\n",
    "for ls in train_labels:\n",
    "    # Appending a None to compensate for speaker added in embeddings\n",
    "    ls = ['None'] + ls\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    for l in ls:\n",
    "        temp.append(np.eye(len(entities))[entities.index(l)])\n",
    "    temp = np.asarray(temp)\n",
    "    train_one_hot.append(temp)\n",
    "\n",
    "print(len(train_input))\n",
    "print(len(train_one_hot))\n",
    "print(len(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_input.npy', train_input)\n",
    "np.save('train_one_hot.npy', train_one_hot)\n",
    "np.save('test_input.npy', test_input)\n",
    "np.save('test_labels_ground_truth.npy', test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
